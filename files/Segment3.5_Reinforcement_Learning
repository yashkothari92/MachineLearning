Its important to point out that Reinforcement learning is describing problem and not a solution.
In the same way that Linear Regression is one soln to the supervised Regression problem. There are many algo that solve RL problem.

In termas of a problem for a Robot
Robot is gonna interact with Environment (Cloud), 
Robot is going to take action that will change the environment. 
It will sense the environment and reason over what it sees and take another action.

In Robotics, we call this sense, think, act cycle.
We will focus on how to do that with Reinforcement Learning

Robot observes environment.
Some form of description of environment comes in. Let's call that State 'S'
Robot has to process that state somehow to determine what to do. We call this policy (pi)
Policy(pi) takes state 's' and outputs an action 'a'. 
Action 'a' affects the environment in some way and changes it

This is kind of circular process, action a is taken into environment, and then environment is transitions (T) to new state.
T(transition fun) which takes previous state(s), and action(a) and moves to a new State(s^)

Everytime Robot is in particular state and it takes an action. There's particular reward associated (with taking that action & that state)
Reward comes into Robot, and Robot keeps this Reward in pocket where it keeps cash.
Robot's objective over time to take actions that maximizes this reward.

S (state of env) that Robot senses to decide what to do. 
It uses policy (pi) to figure out the action (a) should be [pi can be simple look-up table]
Over time, Robot takes action(a), and it gets a Reward(r). 
And it will try to find (pi) that will maximize its reward over time.

Now In terms on trading:
-----------------------
Environment --> Market
Action(a) --> {Buy/Sell/Do Nothing}
State(s) --> Factors about stocks that we might observe and know about (Bollinger band, PE, daily_return ...)
Reward(r) --> return we get for making proper trades {Return from trade, daily_return}

Markov decision Problems
------------------------
> Set of states (s) (comes into Robot)
> set of actions (a) (potential actions can take to act on env)
> Transition Function T[s,a,s*] (this is T within env)
	the probability that if we are in state s and take action a, we will end up in state S*
	3D object - suppose we're in state s, and we take action a. Sum of all next states we might end up in has to sum to one.(probability 1)
> Reward function R[s,a]
	Important componoent of Markov Decision problem: if we take particular state s and take particualr acion a, we get a particular reward  


Now the problem for Reinforcement Learning Algo is to
Find policy Pi(s) that will maximize reward over time.

If we have Transition Fun(T) & Reward Fun (R) there are algo we can unleash to find Optimal Policy(Pi*)
Two of them are 
i) Policy iteration, 
ii) Value iteration

Unknown transitions & rewards
------------------------------
Most of the time we don't have T or R either. 
So Robot/ Trader has to interact with World, observe what happens and work with data to try to build policy

<s1, a1, s1*, r1> 
<s2, a2, s2*, r2> (where s2=s1*)
.
.
.

{trail of experience tuples}
Now with the help of n exp tuples, we can do 2 things with them in order to find that policy pi.

1st approach: Model based
By looking at data over time (experience tuples), we build...
Model of T[s, a, s*]
	R[s, a]

Once we have these models, we can then use
 value iteration or policy iteration to solve the problem.

2nd approach: Model-free
Model-free methods develope a policy just directly be looking at data
(Q-learning)

What to Optimize
----------------
> infinite horizon
Sigma(1 to infinity) ri


> finite horizon
Sigma(1 to n) ri


> discounted reward
Sigma(1 to infinity) (gamma) ^i-1 * ri
[immediae reward, i=0, gamma^0 (1). for the very next step we get r]
[But step after 1, gamma^1; so it devalues reward a little bit]
(usually, 0 < gamma <= 1.0 )

gamma closer to 1, more value rewards in future
gamma closer to 0, less we value rewards in future

if gamma = 0.95, it means each step in future is worth about 5% less than the immediate reward if we got it right away

<> Discounted reward is the method we use in Q-learning.

